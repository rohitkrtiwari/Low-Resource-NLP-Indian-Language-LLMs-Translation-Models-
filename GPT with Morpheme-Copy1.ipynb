{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5910,
     "status": "ok",
     "timestamp": 1745236396476,
     "user": {
      "displayName": "Rohit Tiwari",
      "userId": "01270517727154737436"
     },
     "user_tz": -330
    },
    "id": "Z7ZX0-lTMNT5",
    "outputId": "45692642-a5f7-403c-927e-d0bdc6f19ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized output has been saved to morpheme_tokenized_output_100MB.txt\n"
     ]
    }
   ],
   "source": [
    "# morpheme_tokenizer.py\n",
    "\n",
    "# Expanded Prefixes (unique additions)\n",
    "prefixes = [\n",
    "    'प्र', 'पर', 'अ', 'अन', 'अति', 'स', 'वि', 'उप', 'सं', 'सु', 'दु', 'नि', 'अध', 'अव',\n",
    "    'निर', 'पुन', 'प्रति', 'अभि', 'अधि', 'उत', 'अप', 'अति', 'आ', 'अन्त', 'अन्तर', 'आत्म',\n",
    "    'प्राक्', 'पुरा', 'सह', 'स्व', 'सम', 'वि', 'अध', 'अनु', 'अभि', 'अव', 'आ', 'उद्', 'नि',\n",
    "    'परा', 'प्र', 'सम्', 'सु', 'दु', 'निर', 'परि', 'प्रति', 'वि', 'आ', 'अ', 'अन', 'अध',\n",
    "    'अन्त', 'अन्तर', 'अप', 'अभि', 'अव', 'आ', 'उत्', 'उप', 'दु', 'नि', 'निर', 'पर', 'परि',\n",
    "    'पुन', 'प्र', 'प्रति', 'वि', 'स', 'सम्', 'सु', 'स्व', 'ह्रस्व', 'दीर्घ', 'पूर्व', 'पश्च',\n",
    "    'उत्तर', 'दक्षिण', 'महा', 'लघु', 'आदि', 'अन्त्य', 'मध्य', 'प्रथम', 'द्वितीय', 'तृतीय'\n",
    "]\n",
    "\n",
    "# Expanded Suffixes for Hindi Wikipedia/encyclopedic content\n",
    "suffixes = [\n",
    "    'ता', 'त्व', 'पन', 'ई', 'इक', 'अक', 'ईय', 'इत', 'अन', 'वान', 'मान', 'हीन', 'वाला',\n",
    "    'वती', 'वान्', 'मती', 'कारी', 'धारी', 'बंद', 'गत', 'युक्त', 'रहित', 'मय', 'वत्', 'शील',\n",
    "    'कर', 'दायक', 'भर', 'पूर्ण', 'युक्त', 'मूलक', 'प्रिय', 'भाषी', 'ज्ञ', 'विद', 'शास्त्री',\n",
    "    'धर्मी', 'वादी', 'तंत्र', 'मंत्र', 'विधि', 'करण', 'गीत', 'सूत्र', 'शास्त्र', 'वृत्ति',\n",
    "    'वाद', 'भाषा', 'लिपि', 'संस्कृति', 'सभ्यता', 'इतिहास', 'भूगोल', 'राजनीति', 'अर्थव्यवस्था',\n",
    "    'सरकार', 'संविधान', 'व्याकरण', 'उच्चारण', 'प्रणाली', 'व्यवस्था', 'पद्धति', 'शैली', 'प्रकार',\n",
    "    'क्षेत्र', 'युग', 'काल', 'समय', 'सदी', 'वर्ष', 'माह', 'दिन', 'संख्या', 'मात्रा'\n",
    "]\n",
    "\n",
    "# Expanded Roots for Hindi Wikipedia/encyclopedic content\n",
    "roots = [\n",
    "    'भाषा', 'हिन्दी', 'संस्कृत', 'लिपि', 'देवनागरी', 'व्याकरण', 'उच्चारण', 'ध्वनि', 'वर्ण',\n",
    "    'शब्द', 'वाक्य', 'अर्थ', 'भाव', 'संस्कृति', 'साहित्य', 'काव्य', 'गद्य', 'कवि', 'लेखक',\n",
    "    'रचना', 'पुस्तक', 'ज्ञान', 'विज्ञान', 'इतिहास', 'भूगोल', 'राजनीति', 'अर्थ', 'व्यवस्था',\n",
    "    'सरकार', 'संविधान', 'कानून', 'अधिकार', 'कर्तव्य', 'राज्य', 'देश', 'राष्ट्र', 'संसद',\n",
    "    'विधान', 'चुनाव', 'प्रधानमंत्री', 'राष्ट्रपति', 'मंत्री', 'विभाग', 'योजना', 'विकास',\n",
    "    'अर्थव्यवस्था', 'उद्योग', 'कृषि', 'वाणिज्य', 'व्यापार', 'निर्यात', 'आयात', 'मुद्रा',\n",
    "    'बैंक', 'वित्त', 'शिक्षा', 'विद्यालय', 'विश्वविद्यालय', 'शिक्षक', 'छात्र', 'पाठ्यक्रम',\n",
    "    'परीक्षा', 'स्वास्थ्य', 'चिकित्सा', 'रोग', 'उपचार', 'अस्पताल', 'डॉक्टर', 'वैज्ञानिक',\n",
    "    'तकनीक', 'अनुसंधान', 'आविष्कार', 'मशीन', 'यंत्र', 'कंप्यूटर', 'इंटरनेट', 'सॉफ्टवेयर',\n",
    "    'मोबाइल', 'यातायात', 'सड़क', 'रेल', 'हवाई', 'जहाज', 'मेट्रो', 'परिवहन', 'संचार',\n",
    "    'टेलीफोन', 'टीवी', 'रेडियो', 'अखबार', 'पत्रकार', 'समाचार', 'मनोरंजन', 'फिल्म', 'गाना',\n",
    "    'नृत्य', 'कला', 'चित्र', 'मूर्ति', 'स्थापत्य', 'भवन', 'मंदिर', 'मस्जिद', 'गिरजा',\n",
    "    'धर्म', 'हिन्दू', 'मुस्लिम', 'सिख', 'ईसाई', 'जैन', 'बौद्ध', 'पूजा', 'प्रार्थना', 'त्योहार',\n",
    "    'उत्सव', 'विवाह', 'संस्कार', 'जन्म', 'मृत्यु', 'परिवार', 'माता', 'पिता', 'पुत्र', 'पुत्री',\n",
    "    'भाई', 'बहन', 'दादा', 'दादी', 'नाती', 'पोती', 'समाज', 'जाति', 'वर्ग', 'समुदाय', 'लोग',\n",
    "    'जनसंख्या', 'नागरिक', 'महिला', 'पुरुष', 'बच्चा', 'युवा', 'वृद्ध', 'किसान', 'मजदूर',\n",
    "    'कर्मचारी', 'अधिकारी', 'नेता', 'अभिनेता', 'गायक', 'लेखक', 'कवि', 'चित्रकार', 'मूर्तिकार',\n",
    "    'वास्तुकार', 'इंजीनियर', 'डॉक्टर', 'शिक्षक', 'वैज्ञानिक', 'खिलाड़ी', 'अध्यापक', 'प्राध्यापक',\n",
    "    'विद्वान', 'पंडित', 'मौलवी', 'पादरी', 'गुरु', 'शिष्य', 'भक्त', 'योगी', 'साधु', 'संन्यासी',\n",
    "    'क्रांतिकारी', 'स्वतंत्रता', 'संग्राम', 'युद्ध', 'शांति', 'सन्धि', 'समझौता', 'संविधान', 'कानून',\n",
    "    'अदालत', 'जज', 'वकील', 'पुलिस', 'सेना', 'नौसेना', 'वायुसेना', 'सैनिक', 'अधिकारी', 'जवान',\n",
    "    'युद्ध', 'शस्त्र', 'बंदूक', 'तोप', 'टैंक', 'जहाज', 'विमान', 'मिसाइल', 'परमाणु', 'ऊर्जा',\n",
    "    'बिजली', 'पेट्रोल', 'डीजल', 'गैस', 'कोयला', 'खनिज', 'सोना', 'चाँदी', 'ताँबा', 'लोहा',\n",
    "    'इमारत', 'मकान', 'फ्लैट', 'बंगला', 'हवेली', 'महल', 'किला', 'स्टेशन', 'हवाईअड्डा', 'बस',\n",
    "    'स्टैंड', 'बाजार', 'दुकान', 'मॉल', 'हॉस्पिटल', 'स्कूल', 'कॉलेज', 'यूनिवर्सिटी', 'पार्क',\n",
    "    'उद्यान', 'चिड़ियाघर', 'संग्रहालय', 'पुस्तकालय', 'अस्पताल', 'दवाखाना', 'दवा', 'गोली',\n",
    "    'इंजेक्शन', 'ऑपरेशन', 'चिकित्सा', 'डॉक्टर', 'नर्स', 'मरीज', 'रोग', 'बुखार', 'खाँसी',\n",
    "    'जुकाम', 'दर्द', 'चोट', 'दुर्घटना', 'मौत', 'जीवन', 'स्वास्थ्य', 'आरोग्य', 'योग', 'प्राणायाम',\n",
    "    'ध्यान', 'आहार', 'भोजन', 'पानी', 'दूध', 'चाय', 'कॉफी', 'रोटी', 'चावल', 'दाल', 'सब्जी',\n",
    "    'फल', 'मिठाई', 'नमक', 'मिर्च', 'मसाला', 'तेल', 'घी', 'मक्खन', 'पनीर', 'दही', 'लस्सी',\n",
    "    'शरबत', 'जूस', 'सूप', 'सलाद', 'अचार', 'चटनी', 'पापड़', 'बिस्कुट', 'केक', 'आइसक्रीम',\n",
    "    'चॉकलेट', 'पान', 'सिगरेट', 'शराब', 'दवा', 'नशा', 'सुधार', 'केंद्र', 'आश्रम', 'मंदिर',\n",
    "    'मस्जिद', 'गुरुद्वारा', 'चर्च', 'धर्मशाला', 'सत्संग', 'भजन', 'कीर्तन', 'प्रवचन', 'उपदेश',\n",
    "    'शिक्षा', 'ज्ञान', 'विद्या', 'बुद्धि', 'मेधा', 'प्रतिभा', 'होशियारी', 'चतुराई', 'समझ',\n",
    "    'बुद्धिमान', 'मूर्ख', 'अज्ञान', 'अंधविश्वास', 'परंपरा', 'रीति', 'रिवाज', 'प्रथा', 'संस्कार',\n",
    "    'विवाह', 'जन्म', 'मृत्यु', 'संस्कार', 'उपनयन', 'मुंडन', 'कर्णछेद', 'विद्यारंभ', 'विवाह',\n",
    "    'अंतिम', 'संस्कार', 'श्राद्ध', 'तर्पण', 'पिंडदान', 'मुक्ति', 'मोक्ष', 'स्वर्ग', 'नरक',\n",
    "    'पुनर्जन्म', 'कर्म', 'भाग्य', 'नियति', 'ईश्वर', 'भगवान', 'अल्लाह', 'वाहेगुरु', 'ईसा',\n",
    "    'मसीह', 'पैगम्बर', 'ऋषि', 'मुनि', 'साधु', 'संत', 'महात्मा', 'गुरु', 'शिष्य', 'भक्त',\n",
    "    'आराधना', 'पूजा', 'इबादत', 'प्रार्थना', 'सत्संग', 'भजन', 'कीर्तन', 'प्रवचन', 'उपदेश',\n",
    "    'धर्म', 'अध्यात्म', 'योग', 'तप', 'साधना', 'ध्यान', 'समाधि', 'मोक्ष', 'निर्वाण', 'शांति',\n",
    "    'आनंद', 'सुख', 'दुख', 'कष्ट', 'पीड़ा', 'वेदना', 'तकलीफ', 'रोग', 'शोक', 'दुःख', 'खुशी',\n",
    "    'आनंद', 'प्रसन्नता', 'हर्ष', 'उल्लास', 'उमंग', 'उत्साह', 'जोश', 'खुशी', 'मस्ती', 'मजा',\n",
    "    'आनंद', 'सुख', 'शांति', 'संतोष', 'तृप्ति', 'परितोष', 'आह्लाद', 'प्रफुल्लता', 'हर्ष',\n",
    "    'उमंग', 'उत्साह', 'जोश', 'खुशी', 'मस्ती', 'मजा', 'आनंद', 'सुख', 'शांति', 'संतोष',\n",
    "    'तृप्ति', 'परितोष', 'आह्लाद', 'प्रफुल्लता', 'हर्ष', 'उमंग', 'उत्साह', 'जोश', 'खुशी',\n",
    "    'मस्ती', 'मजा', 'आनंद', 'सुख', 'शांति', 'संतोष', 'तृप्ति', 'परितोष', 'आह्लाद', 'प्रफुल्लता'\n",
    "]\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize_morpheme(word):\n",
    "    # Try to match prefix\n",
    "    for prefix in prefixes:\n",
    "        if word.startswith(prefix):\n",
    "            word = word[len(prefix):]  # Remove prefix\n",
    "            return [prefix] + tokenize_morpheme(word)\n",
    "\n",
    "    # Try to match suffix\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            word = word[:-len(suffix)]  # Remove suffix\n",
    "            return tokenize_morpheme(word) + [suffix]\n",
    "\n",
    "    # If the word is in roots, return it as a root\n",
    "    if word in roots:\n",
    "        return [word]\n",
    "\n",
    "    # If no match, return the word as it is\n",
    "    return [word]\n",
    "\n",
    "# Function to process the file and tokenize each word\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Read file content\n",
    "        content = file.read()\n",
    "\n",
    "        # Tokenize the content\n",
    "        words = content.split()\n",
    "        tokenized_words = {}\n",
    "\n",
    "        for word in words:\n",
    "            # Apply morpheme tokenizer\n",
    "            morphemes = tokenize_morpheme(word)\n",
    "            tokenized_words[word] = ' + '.join(morphemes)\n",
    "\n",
    "        return tokenized_words\n",
    "\n",
    "# Function to save tokenized words to a file\n",
    "def save_tokenized_output(file_path, output_file):\n",
    "    # Process the input file\n",
    "    tokenized_words = process_file(file_path)\n",
    "\n",
    "    # Open output file for writing\n",
    "    with open(output_file, 'w', encoding='utf-8') as output:\n",
    "        for word, morphemes in tokenized_words.items():\n",
    "            output.write(f\"{word} → {morphemes}\\n\")\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_file.txt' with the path to your input text file\n",
    "    input_file = 'wikipedia_articles_cleaned_100MB.txt'  # Change this path\n",
    "    output_file = 'morpheme_tokenized_output_100MB.txt'  # Output file where tokenized words will be saved\n",
    "\n",
    "    # Save tokenized words to the output file\n",
    "    save_tokenized_output(input_file, output_file)\n",
    "\n",
    "    print(f\"Tokenized output has been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9gvPHkbQ9Zj"
   },
   "source": [
    "TRAINING llM ON BOTH DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1745236399840,
     "user": {
      "displayName": "Rohit Tiwari",
      "userId": "01270517727154737436"
     },
     "user_tz": -330
    },
    "id": "_wrKqPOMRAxo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "\n",
    "def train_tokenizer(file_path, vocab_size=8000, output_path=\"tokenizer.json\"):\n",
    "    from tokenizers import Tokenizer\n",
    "    from tokenizers.models import WordLevel\n",
    "    from tokenizers.trainers import WordLevelTrainer\n",
    "    from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "    trainer = WordLevelTrainer(vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    tokenizer.train([file_path], trainer)\n",
    "    tokenizer.save(output_path)\n",
    "\n",
    "# Train on both tokenized versions\n",
    "#train_tokenizer(\"bpe_tokenized.txt\", output_path=\"bpe_tokenizer.json\")\n",
    "train_tokenizer(\"morpheme_tokenized_output_100MB.txt\", output_path=\"morph_tokenizer_100MB.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7176,
     "status": "ok",
     "timestamp": 1745236410789,
     "user": {
      "displayName": "Rohit Tiwari",
      "userId": "01270517727154737436"
     },
     "user_tz": -330
    },
    "id": "iw6gAsRLRd7p",
    "outputId": "f8dd787c-345a-4f92-8097-87afe1e8712a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 8178047\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_file=\"morph_tokenizer_100MB.json\")\n",
    "\n",
    "# Load your text\n",
    "with open(\"wikipedia_articles_cleaned_100MB.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Encode the full text\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "\n",
    "# Create a simple dataset that chunks the tokenized text\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, block_size):\n",
    "        self.tokens = tokens\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.block_size\n",
    "        end = start + self.block_size\n",
    "        x = torch.tensor(self.tokens[start:end], dtype=torch.long)\n",
    "        y = torch.tensor(self.tokens[start + 1:end + 1], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 10046,
     "status": "ok",
     "timestamp": 1745236422262,
     "user": {
      "displayName": "Rohit Tiwari",
      "userId": "01270517727154737436"
     },
     "user_tz": -330
    },
    "id": "Yo1XYKCjMNSS"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# Build a GPT2 model from scratch\n",
    "config = GPT2Config(\n",
    "    vocab_size=12_000,\n",
    "    n_positions=256,\n",
    "    n_ctx=256,\n",
    "    n_embd=384,\n",
    "    n_layer=8,\n",
    "    n_head=6,\n",
    "    resid_pdrop=0.1,             # Dropout for residual connections\n",
    "    attn_pdrop=0.1,              # Dropout for attention layers\n",
    "    embd_pdrop=0.1,              # Dropout for embeddings\n",
    "    # Optimization\n",
    "    activation_function=\"gelu\",\n",
    "    use_cache=False,             \n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209413,
     "status": "ok",
     "timestamp": 1745236634019,
     "user": {
      "displayName": "Rohit Tiwari",
      "userId": "01270517727154737436"
     },
     "user_tz": -330
    },
    "id": "2pHhT27AMNST",
    "outputId": "f21940ed-1a1d-42dd-b5ea-4091d523e920"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|                                                                              | 0/998 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 1/20: 100%|█████████████████████████████████████████████████████████| 998/998 [02:07<00:00,  7.84it/s, loss=4.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 4.3989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|█████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.77it/s, loss=4.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 4.0859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|█████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.77it/s, loss=3.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 3.9641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.77it/s, loss=3.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 3.8856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|█████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.77it/s, loss=3.82]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 3.8263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|█████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.77it/s, loss=3.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 3.7766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|█████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.77it/s, loss=3.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 3.7349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.77it/s, loss=3.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 3.6968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|█████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.76it/s, loss=3.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 3.6623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.76it/s, loss=3.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 3.6309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.76it/s, loss=3.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 3.6004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.77it/s, loss=3.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 3.5725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.76it/s, loss=3.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 3.5455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.76it/s, loss=3.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 3.5198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.77it/s, loss=3.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 3.4955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.77it/s, loss=3.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 3.4718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.76it/s, loss=3.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss: 3.4490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.75it/s, loss=3.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 3.4270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.76it/s, loss=3.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss: 3.4066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|████████████████████████████████████████████████████████| 998/998 [02:08<00:00,  7.76it/s, loss=3.38]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss: 3.3866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "dataset = TextDataset(tokens, block_size=256)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "\n",
    "# Gradient clipping to avoid explosions\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Train manually\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    for x, y in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        outputs = model(x, labels=y)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Clip gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1745236647158,
     "user": {
      "displayName": "Rohit Tiwari",
      "userId": "01270517727154737436"
     },
     "user_tz": -330
    },
    "id": "pO4BsbktMNSU",
    "outputId": "3bd5c597-e468-463e-ecd9-669dafec3c6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2_morph_manual_100MB\\\\tokenizer_config.json',\n",
       " 'gpt2_morph_manual_100MB\\\\special_tokens_map.json',\n",
       " 'gpt2_morph_manual_100MB\\\\vocab.json',\n",
       " 'gpt2_morph_manual_100MB\\\\added_tokens.json',\n",
       " 'gpt2_morph_manual_100MB\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"gpt2_morph_manual_100MB\")\n",
    "tokenizer.save_pretrained(\"gpt2_morph_manual_100MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0zdWELvzMNSW",
    "outputId": "966fdfdd-5cb2-4d6b-de43-b54f9d32b431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "द्वारा गृह युद्ध के बीच में गया । ने को सबसे बताया पूर्व अपने की का करने लिए जिसमें व्यक्ति या शामिल व भी हो यह रूप एक है हर और से पर करता कि ही इस बिना राष्ट्र शासन मुक्ति किया सके नहीं वह\n"
     ]
    }
   ],
   "source": [
    "input_text = \"अब्राहम लिंकन द्वारा अमरीकी गृह युद्ध के बीच में गुलामों:\"\n",
    "\n",
    "# Tokenize and move tensors to the same device as the model\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Add `.to(device)`\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,  # Ensure pad_token_id is set\n",
    "    no_repeat_ngram_size=1\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
