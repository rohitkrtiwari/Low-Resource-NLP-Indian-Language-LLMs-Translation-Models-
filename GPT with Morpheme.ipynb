{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQCGpzmmy1-L",
    "outputId": "a0b4c13a-3367-4308-fefc-7916bda70dda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Hindi text saved to hindi_wikipedia_corpus_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Path to input and output files\n",
    "input_file = \"hindi_wikipedia_corpus.txt\"\n",
    "output_file = \"hindi_wikipedia_corpus_cleaned.txt\"\n",
    "\n",
    "# Devanagari Unicode range: U+0900 to U+097F\n",
    "# Optional: Add some punctuation if needed (like । or whitespace)\n",
    "devanagari_pattern = re.compile(r'^[\\u0900-\\u097F]+$')\n",
    "def is_hindi_word(word):\n",
    "    return devanagari_pattern.match(word) is not None\n",
    "\n",
    "cleaned_lines = []\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "        words = line.strip().split()\n",
    "        hindi_words = [word for word in words if is_hindi_word(word)]\n",
    "        if hindi_words:\n",
    "            cleaned_lines.append(\" \".join(hindi_words))\n",
    "\n",
    "# Save cleaned lines to a new file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in cleaned_lines:\n",
    "        outfile.write(line + \"\\n\")\n",
    "\n",
    "print(f\"Cleaned Hindi text saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5910,
     "status": "ok",
     "timestamp": 1745236396476,
     "user": {
      "displayName": "Rohit Tiwari",
      "userId": "01270517727154737436"
     },
     "user_tz": -330
    },
    "id": "Z7ZX0-lTMNT5",
    "outputId": "45692642-a5f7-403c-927e-d0bdc6f19ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized output has been saved to tokenized_output.txt\n"
     ]
    }
   ],
   "source": [
    "# morpheme_tokenizer.py\n",
    "\n",
    "# Expanded Prefixes (unique additions)\n",
    "prefixes = [\n",
    "    'अ', 'अन', 'अध', 'अति', 'पर', 'स', 'व', 'प्र', 'उ', 'सभी', 'उप', 'वि', 'संप', 'प्रौ',\n",
    "    'अधिकार', 'आ', 'न', 'सम', 'अंतर', 'बिना', 'सदृश', 'पार', 'उदाहरण', 'रूप', 'विक', 'संपर्क',\n",
    "    'विवेक', 'क', 'मुख्य', 'प्रकृति', 'तंत्र', 'धर्म', 'सामाजिक', 'विज्ञान', 'विशेष', 'अधिकार',\n",
    "    'प्रति', 'अधिवेशन', 'संचालन', 'संविधान', 'समाज', 'प्रतिनिधि', 'संचालित', 'अंतरिक्ष',\n",
    "    'विज्ञापन', 'आधिकारिक', 'आध्यात्मिक', 'कृपया', 'संस्कृत', 'दृश्य', 'पारंपरिक', 'स्वदेशी',\n",
    "    'अंतरराष्ट्रीय', 'न्यायिक', 'प्राकृतिक', 'समान', 'समानांतर', 'संकुचित', 'विकसीत', 'साक्षात्कार',\n",
    "    'उधार', 'उत्साहित', 'सजग', 'आत्म', 'विश्वास', 'प्राकृतिक', 'विकसित'\n",
    "]\n",
    "\n",
    "# Expanded Suffixes (unique additions)\n",
    "suffixes = [\n",
    "    'ता', 'वाला', 'वाली', 'ने', 'रूप', 'पन', 'इ', 'नुमा', 'ण', 'वृत्ति', 'वाद', 'शक्ति', 'रण',\n",
    "    'कता', 'धन', 'सिद्ध', 'त्व', 'ता', 'करण', 'ज्ञान', 'काल', 'शास्त्र', 'ई', 'साधना', 'सार',\n",
    "    'देश', 'मूल्य', 'वृद्धि', 'वाला', 'प्रकार', 'भावना', 'आत्म', 'शांति', 'समान', 'वर्धन',\n",
    "    'हीन', 'मूल', 'निवासी', 'शुल्क', 'विधि', 'कारण', 'वीर', 'देश', 'अधिकार', 'भविष्य', 'प्रभाव',\n",
    "    'स्वीकृति', 'त्याग', 'योजना', 'पुरस्कार', 'निर्णय', 'जन', 'संतुलन', 'मान्यता', 'विश्व', 'संपत्ति',\n",
    "    'सिद्धांत', 'समाजवादी', 'आधुनिक', 'सुरक्षा', 'संगठित', 'निष्ठा', 'परिस्थिति', 'विकास', 'शक्ति',\n",
    "    'साक्षात्कार', 'प्रशासन', 'रचनात्मक', 'समर्थ', 'विवेचना', 'शिकायत', 'मुलायम', 'विवाह',\n",
    "    'महत्व', 'समझौता', 'समर्पण', 'स्वास्थ्य', 'पदवी', 'लाभ', 'लक्षण', 'नौकरी', 'योजना'\n",
    "]\n",
    "\n",
    "# Expanded Roots (unique additions)\n",
    "roots = [\n",
    "    'भारत', 'शक्ति', 'समाज', 'धर्म', 'विज्ञान', 'ज्ञान', 'प्रकृति', 'जीवन', 'कला', 'शांति', 'प्यार',\n",
    "    'सपना', 'विकास', 'आत्मा', 'मूल्य', 'रक्षा', 'शौर्य', 'विज्ञानी', 'वृत्ति', 'संतोष',\n",
    "    'राज', 'नेता', 'संत', 'योग', 'विपक्ष', 'विजेता', 'लक्ष्य', 'मूल', 'न्याय', 'सत्यमेव',\n",
    "    'मित्र', 'कर्म', 'शिक्षा', 'श्रम', 'विधि', 'रचनात्मक', 'जिंदगी', 'संगीत', 'दर्शन',\n",
    "    'संविधान', 'लोक', 'राजनीति', 'प्रशासन', 'स्वतंत्रता', 'संस्कार', 'मुलायम', 'संघ',\n",
    "    'विरासत', 'अनुराग', 'भावना', 'रचनात्मकता', 'समस्या', 'समाधान', 'आत्मनिर्भर', 'अभियान',\n",
    "    'उत्साह', 'सपने', 'भविष्य', 'शिक्षक', 'शिक्षिका', 'विद्यालय', 'नौकरी', 'स्वास्थ्य', 'समाजसेवा',\n",
    "    'विकिरण', 'विश्वविद्यालय', 'संस्था', 'अधिकार', 'संगठन', 'संचार', 'मूल', 'समाजशास्त्र', 'अर्थशास्त्र',\n",
    "    'शिक्षण', 'अध्ययन', 'समाजवादी', 'विपरीत', 'सामाजिक', 'कर्मचारी', 'प्रारंभ', 'शुद्धता', 'विवाह',\n",
    "    'तंत्रज्ञान', 'विपणन', 'नैतिकता', 'संस्कार', 'निर्माण', 'स्वतंत्रता', 'ध्वनि', 'आदर्श'\n",
    "]\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize_morpheme(word):\n",
    "    # Try to match prefix\n",
    "    for prefix in prefixes:\n",
    "        if word.startswith(prefix):\n",
    "            word = word[len(prefix):]  # Remove prefix\n",
    "            return [prefix] + tokenize_morpheme(word)\n",
    "\n",
    "    # Try to match suffix\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            word = word[:-len(suffix)]  # Remove suffix\n",
    "            return tokenize_morpheme(word) + [suffix]\n",
    "\n",
    "    # If the word is in roots, return it as a root\n",
    "    if word in roots:\n",
    "        return [word]\n",
    "\n",
    "    # If no match, return the word as it is\n",
    "    return [word]\n",
    "\n",
    "# Function to process the file and tokenize each word\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Read file content\n",
    "        content = file.read()\n",
    "\n",
    "        # Tokenize the content\n",
    "        words = content.split()\n",
    "        tokenized_words = {}\n",
    "\n",
    "        for word in words:\n",
    "            # Apply morpheme tokenizer\n",
    "            morphemes = tokenize_morpheme(word)\n",
    "            tokenized_words[word] = ' + '.join(morphemes)\n",
    "\n",
    "        return tokenized_words\n",
    "\n",
    "# Function to save tokenized words to a file\n",
    "def save_tokenized_output(file_path, output_file):\n",
    "    # Process the input file\n",
    "    tokenized_words = process_file(file_path)\n",
    "\n",
    "    # Open output file for writing\n",
    "    with open(output_file, 'w', encoding='utf-8') as output:\n",
    "        for word, morphemes in tokenized_words.items():\n",
    "            output.write(f\"{word} → {morphemes}\\n\")\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_file.txt' with the path to your input text file\n",
    "    input_file = 'hindi_wikipedia_corpus_cleaned.txt'  # Change this path\n",
    "    output_file = 'tokenized_output.txt'  # Output file where tokenized words will be saved\n",
    "\n",
    "    # Save tokenized words to the output file\n",
    "    save_tokenized_output(input_file, output_file)\n",
    "\n",
    "    print(f\"Tokenized output has been saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IQTKIAzMxWF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9gvPHkbQ9Zj"
   },
   "source": [
    "TRAINING llM ON BOTH DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1745236399840,
     "user": {
      "displayName": "Rohit Tiwari",
      "userId": "01270517727154737436"
     },
     "user_tz": -330
    },
    "id": "_wrKqPOMRAxo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "\n",
    "def train_tokenizer(file_path, vocab_size=30000, output_path=\"tokenizer.json\"):\n",
    "    from tokenizers import Tokenizer\n",
    "    from tokenizers.models import WordLevel\n",
    "    from tokenizers.trainers import WordLevelTrainer\n",
    "    from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "    trainer = WordLevelTrainer(vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    tokenizer.train([file_path], trainer)\n",
    "    tokenizer.save(output_path)\n",
    "\n",
    "# Train on both tokenized versions\n",
    "#train_tokenizer(\"bpe_tokenized.txt\", output_path=\"bpe_tokenizer.json\")\n",
    "train_tokenizer(\"tokenized_output.txt\", output_path=\"morph_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7176,
     "status": "ok",
     "timestamp": 1745236410789,
     "user": {
      "displayName": "Rohit Tiwari",
      "userId": "01270517727154737436"
     },
     "user_tz": -330
    },
    "id": "iw6gAsRLRd7p",
    "outputId": "f8dd787c-345a-4f92-8097-87afe1e8712a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 271285\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_file=\"morph_tokenizer.json\")\n",
    "\n",
    "# Load your text\n",
    "with open(\"hindi_wikipedia_corpus_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Encode the full text\n",
    "tokens = tokenizer.encode(text)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "\n",
    "# Create a simple dataset that chunks the tokenized text\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, block_size):\n",
    "        self.tokens = tokens\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) // self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.block_size\n",
    "        end = start + self.block_size\n",
    "        x = torch.tensor(self.tokens[start:end], dtype=torch.long)\n",
    "        y = torch.tensor(self.tokens[start + 1:end + 1], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 10046,
     "status": "ok",
     "timestamp": 1745236422262,
     "user": {
      "displayName": "Rohit Tiwari",
      "userId": "01270517727154737436"
     },
     "user_tz": -330
    },
    "id": "Yo1XYKCjMNSS"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# Build a GPT2 model from scratch\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=128,\n",
    "    n_ctx=128,\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=4\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209413,
     "status": "ok",
     "timestamp": 1745236634019,
     "user": {
      "displayName": "Rohit Tiwari",
      "userId": "01270517727154737436"
     },
     "user_tz": -330
    },
    "id": "2pHhT27AMNST",
    "outputId": "f21940ed-1a1d-42dd-b5ea-4091d523e920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/530 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "100%|██████████| 530/530 [00:10<00:00, 49.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.3840\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 50.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.0616\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:09<00:00, 53.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.6113\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 52.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.5115\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 51.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.1297\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 51.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.2287\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 51.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.1076\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 51.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.2100\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 51.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.6979\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 52.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.8840\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 51.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.5388\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 52.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.0882\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 51.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.2726\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 51.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.9209\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:13<00:00, 39.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.7938\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 50.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.0931\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 51.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.5307\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 51.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.2639\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 50.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.2792\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 530/530 [00:10<00:00, 51.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "dataset = TextDataset(tokens, block_size=128)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Train manually\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for x, y in tqdm(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        outputs = model(x, labels=y)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1745236647158,
     "user": {
      "displayName": "Rohit Tiwari",
      "userId": "01270517727154737436"
     },
     "user_tz": -330
    },
    "id": "pO4BsbktMNSU",
    "outputId": "3bd5c597-e468-463e-ecd9-669dafec3c6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2_morph_manual/tokenizer_config.json',\n",
       " 'gpt2_morph_manual/special_tokens_map.json',\n",
       " 'gpt2_morph_manual/vocab.json',\n",
       " 'gpt2_morph_manual/added_tokens.json',\n",
       " 'gpt2_morph_manual/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"gpt2_morph_manual\")\n",
    "tokenizer.save_pretrained(\"gpt2_morph_manual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zdWELvzMNSW",
    "outputId": "966fdfdd-5cb2-4d6b-de43-b54f9d32b431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "अब्राहम [UNK] द्वारा अमरीकी गृह युद्ध के बीच में [UNK] [UNK] → + अगरबत्ती अ गरबत्ती ण गयी पत्रिकायें उ ठाना प्र शांत व िधानमंडल न ्याय नुमा स त्यदीपक । पहले पुरातात्त्विक ज्ञान शैलाश्रयों ता है युक्त आ म ने सार्वभौमिक जाल भी रहा वह क ट वाद समीक्षाएँ\n"
     ]
    }
   ],
   "source": [
    "input_text =  \"अब्राहम लिंकन द्वारा अमरीकी गृह युद्ध के बीच में गुलामों:\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Set pad_token_id to eos_token_id (or define your own pad_token_id if you have one)\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    no_repeat_ngram_size=1\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_J85gWqMNSX"
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2_morph_manual\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2_morph_manual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iOOUaNNMNSY",
    "outputId": "4db2dcce-7d83-4dd5-e418-b63b7cd9066e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "कुछ उदाहरण दें । ला अली बे की मु फ़ ज टु ने के में को दा री जो और से तक अ हुई उनकी पर एक तो े ली आय ट्ट या ट ए क़ ठी जीते केवल प्राप्तक यह निपुण शैली नहीं है कानो अपनी थी उन्होंने ज्यादा\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"कुछ उदाहरण दें।\", return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Set pad_token_id to eos_token_id (or define your own pad_token_id if you have one)\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    no_repeat_ngram_size=1\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
